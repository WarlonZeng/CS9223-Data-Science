{
    "collab_server" : "",
    "contents" : "# Warlon Zeng, N11183332\n\n# Set working Directory\n\nsetwd(\"C:/Users/Warlon/Documents/R/hw1\")\n\n# ----------------------------------------------------------------\n\n# Q1\n\ndf <- read.csv(\"data.txt\") # read in data.txt stored locally in same folder as hw.R. df is short for data frame.\n\nregion1 <- df$HHS.Region.1..CT..ME..MA..NH..RI..VT. # $ sign is an operator to access column. After $hhs I have auto completion to find Region.\nregion10 <- df$HHS.Region.10..AK..ID..OR..WA. # After $hhs I have auto completion to find Region.\nsummary(region1) # print descriptive statistics\nsummary(region10) # print descriptive statistics\ndate <- as.Date(df$Date) # get date to numeric\n\nplot(date, region1, type=\"l\", col = \"red\", main=\"Flu Trends of Region 1 (Red) and Region 10 (Blue)\", xlab = \"Date\", ylab = \"Search Queries\") # parameters here: http://stat.ethz.ch/R-manual/R-patched/library/graphics/html/plot.html\nlines(date, region10, col=\"blue\") # multiple lines in one graph here: http://stackoverflow.com/questions/2564258/plot-two-graphs-in-same-plot-in-r\n\n# Manually removed this piece of text from data.txt (downloaded data.txt @ http://www.google.org/flutrends/about/data/flu/us/data.txt)\n# Removed introductory paragraphs at beginning.\n# we answer the task by summary() because descriptive statistics are mean, median, mode, etc. things of that nature.\n# we also answer the task by plotting a visual, where it may be better to look at where the data spikes in the event of outliers.\n# one observation is that around the start of every year, when it is coldest, search queries spike.\n# region 10 generally queries more than region 1 overall.\n\n# if you don't have zoo package, perform install.packages('zoo') first. \nrequire(zoo)\n\nphoenix <- df$Phoenix..AZ # phoenix has no NA's so we do not need to clean it\ndf_phoenix <- data.frame(phoenix)\n#plot(date, df_phoenix$phoenix, type = 'l', col = 'green')\n\ntempe <- df$Tempe..AZ # tempe has no NA's so we do not need to clean it\ndf_tempe <- data.frame(tempe)\n#plot(date, df_tempe$tempe, type = 'l', col = 'blue')\n\ntucson <- df$Tucson..AZ # tempe has no NA's so we do not need to clean it\ndf_tucson <- data.frame(tucson)\n#plot(date, df_tucson$tucson, type = 'l', col = 'purple')\n\n\ndf_mesa <- na.spline(df$Date, df$Mesa..AZ)\nplot(date, df$Mesa..AZ, type = 'l', col = 'red')\n\n\nmesa <- df$Mesa..AZ # visually means nothing in table\ndf_mesa <- data.frame(mesa) # data frame with NA's\ndf_mesa$mesa[1] <- mean(c(df_phoenix$phoenix[1], df_tempe$tempe[1], df_tucson$tucson[1]))\ndf_mesa$mesa[11] <- mean(c(df_phoenix$phoenix[11], df_tempe$tempe[11], df_tucson$tucson[11]))\ndf_mesa$mesa[21] <- mean(c(df_phoenix$phoenix[21], df_tempe$tempe[21], df_tucson$tucson[21]))\ndf_mesa <- na.approx(df_mesa) # i am using linear interpolation instead of polynomial interpolation to fill in missing values\n\nscottsdale <- df$Scottsdale..AZ \ndf_scottsdale <- data.frame(scottsdale)\ndf_scottsdale$scottsdale[1] <- mean(c(df_phoenix$phoenix[1], df_tempe$tempe[1], df_tucson$tucson[1])) # lowest point in comparison within same class \"AZ\"\ndf_scottsdale$scottsdale[11] <- mean(c(df_phoenix$phoenix[11], df_tempe$tempe[11], df_tucson$tucson[11])) # highest point in comparison within same class \"AZ\"\ndf_scottsdale$scottsdale[21] <- mean(c(df_phoenix$phoenix[21], df_tempe$tempe[21], df_tucson$tucson[21]))\ndf_scottsdale <- na.approx(df_scottsdale) # i am using linear interpolation instead of polynomial interpolation to fill in missing values\n\nplot(date, df_scottsdale, type = 'l', col = 'black', main=\"Flu Trends of AZ cities\", xlab = \"Date\", ylab = \"Search Queries\")\nlines(date, df_mesa, type = 'l', col = 'red')\nlines(date, df_tucson$tucson, type = 'l', col = 'purple')\nlines(date, df_tempe$tempe, type = 'l', col = 'blue')\nlines(date, df_phoenix$phoenix, type = 'l', col = 'green')\n\n# we already know there are missing data so we will use zoo package to deal with it. \n# zoo has spline, a polynomial interpolation. good for smooth curves. not perfect, but suitable.\n# only mesa and scottsdale are missing values, so we will simply fill them in with comparable data from those who have it (phoenix, tempe, tuscon)\n# plotting the graph indicates little error in cleaning data, but can be further fine tuned. i used the mean to restore/guess previous peak values and their rise and fall.\n# i used linear instead of polynomial because the data spikes in a spikey fashion, not smoothy down.\n\n#install.packages(\"XML\")\nrequire(XML)\n#install.packages(\"RCurl\")\nrequire(RCurl)\n\nurl = getURL(\"http://www.infoplease.com/us/states/population-by-rank.html\") # http://www.infoplease.com/us/states/population-by-rank.html\npop2015 <- readHTMLTable(url, header = TRUE, which = 1)\ncolnames(pop2015) <- c(\"State\", \"July 2015. pop\") # rename the columns, can be manual -- not computationally expensive.\npop2015 <- pop2015[-52,] # delete the total population row, don't need it.\n\npopQuery <- data.frame(\"population\" = integer(0), \"queries\" = integer(0), \"state\" = character(0), stringsAsFactors=FALSE)\n\nfor (i in 1:51) { # all 50 states\n  state <- pop2015$State[i]\n  state <- sub(\" \", \".\", toString(state))\n  if (state == \"DC\") {\n    state = \"District.of.Columbia\" # matching\n  }\n  queryMax <- max(df[589:620, state]) # 2015JAN-DEC, MAX\n  newRow <- data.frame(\"population\" = pop2015$`July 2015. pop`[i], \"queries\" = queryMax, \"state\" = state)\n  popQuery <- rbind(popQuery, newRow)\n}\n\nsortedPop <- data.frame(\"population\" = integer(0), \"queries\" = integer(0), \"state\" = character(0), stringsAsFactors=FALSE)\n# sort into ascending order\nfor (i in 51:1) {\n  newRow <- data.frame(\"population\" = popQuery$population[i], \"queries\" = popQuery$queries[i], \"state\" = popQuery$state[i])\n  #sortedPop <- rbind(sortedPop, newRow1)\n  #newRow2 <- data.frame(\"queries\" = popQuery$queries[i])\n  #sortedPop <- rbind(sortedPop, newRow2)\n  #newRow3 <- data.frame(\"state\" = popQuery$state[i])\n  sortedPop <- rbind(sortedPop, newRow)\n}\n\nplot(sortedPop$population, sortedPop$queries, type = 'l', col = 'black', main=\"Population vs. Queries\", xlab = \"Population\", ylab = \"Search Queries\")\n#model <- lm(sortedPop$population ~ sortedPop$queries)\n#summary(model)\n\n#summary(sortedPop)\n#summary(sortedPop$population)\n#summary(sortedPop$queries)\n\n# source: http://www.infoplease.com/us/states/population-by-rank.html\n# the most recent year is 2015. 2016 did not finish yet. \n# the population for one year remains the same throughout that year. so the population recorded in 2015 will be 1 number.\n# in conclusion: there is little evidence to support population having a strong relationship with flu queries in the year 2015.\n# points are all spread  and no clear pattern. \n# data is dependent on categorical since we are using single data points in an attempt to explain flu queries over a year\n# we'd have to resort to binning and such smooth the data.\n\ndf2 <- read.csv(\"data2.txt\") # read in data.txt stored locally in same folder as hw.R. df is short for data frame. Remove beginning lines.\nlats <- data.frame(\"latitude\" = numeric(0), \"country\" = character(0))\nnewRow <- data.frame(\"latitude\" = -38.416097, country = \"Argentina\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = -25.274398, country = \"Australia\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = 47.516231, country = \"Austria\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = 50.503887, country = \"Belgium\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = -16.290154, country = \"Bolivia\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = -14.235004, country = \"Brazil\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = 42.733883, country = \"Bulgaria\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = 56.130366, country = \"Canada\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = -35.675147, country = \"Chile\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = -46.227638, country = \"France\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = 51.165691, country = \"Germany\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = 47.162494, country = \"Hungary\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = 36.204824, country = \"Japan\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = 23.634501, country = \"Mexico\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = 52.132633, country = \"Netherlands\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = -40.900557, country = \"New Zealand\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = 60.472024, country = \"Norway\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = -23.442503, country = \"Paraguay\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = -9.189967, country = \"Peru\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = 51.919438, country = \"Poland\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = 45.943161, country = \"Romania\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = 61.52401, country = \"Russia\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = -30.559482, country = \"South Africa\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = 40.463667, country = \"Spain\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = 60.128161, country = \"Sweden\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = 46.818188, country = \"Switzerland\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = 48.379433, country = \"Ukraine\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = 37.09024, country = \"United States\")\nlats <- rbind(lats, newRow)\nnewRow <- data.frame(\"latitude\" = -32.522779, country = \"Uruguay\")\nlats <- rbind(lats, newRow)\n\nlatQuery <- data.frame(\"latitude\" = numeric(0), \"queries\" = integer(0), \"country\" = character(0))\n\nfor (i in 1:29) {# 29 countries\n  country <- lats$country[i]\n  country <- sub(\" \", \".\", toString(country))\n  queryMax2 <- max(df2[628:659, country]) # 2015JAN-DEC, MAX\n  newRow <- data.frame(\"latitude\" = lats$latitude[i], \"queries\" = queryMax2, \"country\" = country)\n  latQuery <- rbind(latQuery, newRow)\n}\n\nplot(latQuery$latitude, latQuery$queries, type = 'p', col = 'black', main=\"Latitudes vs. Queries\", xlab = \"Latitude\", ylab = \"Search Queries\")\n\nmodel <- lm(latQuery$latitude ~ latQuery$queries)\nsummary(model)\n\n# i used http://developers.google.com/public-data/docs/canonical/countries_csv for the latitudes. the data set was small so i manually transferred the data.\n# there is absolutely no relationalship with latitudes and search queries for their countries. \n# the multiple R-squared value comes out 0.03837, and even if i had a typo in typing the data in, there is nothing in this\n# data set, visually or statistically, defining a relationship. \n\n# Q2\n\ndf3 <- df\nn <- 4\ndf4 <- aggregate(x = df3, by = list(gl(ceiling(nrow(df3)/n), n)[1:nrow(df3)]), FUN = mean) # aggregate averages of rows in every n specified by 1:nrow(..) and in fun = mean\n\ndf4[\"Group.1\"] = NULL\n\nk <- 1\nl <- 1\nfor (j in 1:620) { # j goes from 1 to 620\n  if (k == 4){\n    #df4$Date[] <- df3$Date[i]\n    \n    df4$Date[l] <- as.Date(df3$Date[j]) # i converted into numerical for graphicability\n    #df4$Date[l] <- date[j] # same thing\n    #x <- data.frame(\"Date\" = df3$Date[j]) # this is how it should look like\n    #df4$Date[l] <- data.frame(\"Date\" = df3$Date[j]) # formatting gets weird here\n    k <- 1 # k is just a resetter for n=4\n    l <- l + 1 # l goes from 1 to 155\n  }\n  else {\n    k <- k + 1\n  }\n}\n\nplot(df4$Date, df4$HHS.Region.1..CT..ME..MA..NH..RI..VT., type=\"l\", col = \"blue\", main=\"Flu Trends of Region 1 aggregated monthly\", xlab = \"Date\", ylab = \"Search Queries\")\n\nmodel <- lm(df4$Date ~ df4$HHS.Region.1..CT..ME..MA..NH..RI..VT.)\nsummary(model)\n\n# based on the graph, there is not much difference. the sample size was cut short to 155 (n=4) but \n# the general trend and shape remains the same. statistics are also similar. \n\n# Q3\n\nurl2 <- getURL(\"http://www.cdc.gov/mmwr/preview/mmwrhtml/mm6401a4.htm?s_cid=mm6401a4_w\") # from assignments linking to this website \ntables1 <- readHTMLTable(url2, header = TRUE, which = 1, stringASFactors=F) # 1st table in the html\ntables2 <- readHTMLTable(url2, header = TRUE, which = 2, stringASFactors=F) # 2nd table in the hmtl\n\n# recall that i used require() to get XML and RCurl packages for getting 2015 year census population of states. \n# i already installed them earlier.\n\nurl3 <- getURL(\"http://example.webscraping.com/view/Sweden-219\")\ntables3 <- readHTMLTable(url3, header = TRUE, which = 1, stringASFactors=F) # 1st table in the html\n\n# i webscrapped an unique page on the web... that uniquely gives webscrapping examples in its own unique way.\n# did GoViral study",
    "created" : 1475151658785.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1011510261",
    "id" : "58AB58C3",
    "lastKnownWriteTime" : 1475345673,
    "last_content_update" : 1475345673811,
    "path" : "~/R/hw1/hw1_Warlon.R",
    "project_path" : "hw1_Warlon.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}